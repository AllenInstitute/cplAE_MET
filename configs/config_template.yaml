---
# Config file for training MET autoencoder (triple modality)
# Modality abbreviation key: T -> transcriptomic, E -> electro-physiological, M -> morphological

# Set device:
device: "cpu"

# Set paths:
data_file: data/raw/MET_M120x4_50k_4Apr23.mat # Location of MATLAB file containing processed input data

# Set experiment/optimization properties:
experiment: autoencoder # Type of experiment
modalities: [T, E, M] # Which modalities to use in the experiment
num_epochs: 5000 # Number of epochs to train
latent_dim: 3 # Dimension of latent space
batch_size: 1000 # Batch size used during training
learning_rate: 0.001 # Learning rate for the ADAM optimizer
check_step: 100 # How often to save model checkpoints

# Set validation parameters:
folds: 10 # Number of folds to use for cross-validation
fold_list: [] # Which folds to run in this experiment (empty list is short for all folds)
val_split: 0.25 # Fraction of data to use for validation (ignored if folds > 0)
seed: 42 # Random seed to use for validation splits

# Set parameters for early stopping:
patience: .inf # How long to wait for improvement of loss
improvement_frac: 0.01 # How much improvement needs to occur for stoppage to be delayed

# Set parameters for gradient freezing:
freeze_modules: ["M_enc"] # Which modules to freeze (format "modality_enc" or "modality_dec")
freeze_patience: 100 # How long to wait for loss improvement
freeze_losses: ["T-M"] # Which losses to sum together and monitor (specify based on config key)

# Set criteria for cell specimen inclusion in experiment
# Note that these are applied after the stratified test-validation split
select:
  platforms: ["patchseq"]

# Set modality-type sample fraction for each batch ("native" uses frequency found in data)
# Note that these fractions are computed after the filtering criteria in the "select" field above
modal_frac:
  # Unimodal
  T: native
  E: native
  M: native
  # Bimodal
  TE: native
  TM: native
  EM: native
  # Trimodal
  MET: native

# Set auto-encoder architecture
T_hidden: [256, 128] # Hidden layer output size for transcriptomic arm
E_hidden: [128, 64] # Hidden layer output size for electro-physiological arm
M_conv: [[10, 1, 10], [10, 1, 10]] # (kernel length, stride, channels) for morphological convolutions
M_hidden: [10] # Hidden layer sizes for morphological arm

# Set augmention/regularization parameters
T_dropout: 0.5 # Dropout fraction for transcriptomic arm
E_dropout: 0.1 # Dropout fraction for electro-physiological arm
M_dropout: 0.0 # Dropout fraction for morphological arm
gauss_var_frac: 0.05   # Fraction of variance to insert as augmentation for electro-physiological arm
encoder_cross_grad: False # Whether cross-reconstruction gradient should propagate back to encoder

# Specify metric to use for reconstruction losses
losses:
  T: r2
  E: r2
  M: mse

# Set loss weights (used when computing the combined loss scalar)

# Single letters indicates within-modality reconstruction loss:
T: 1.0   
E: 1.0   
M: 1.0  
# A-B is the coupling loss between latent spaces of modality A and B, with the gradient only propogated to B
E-T: 1.0  
E-M: 1.0
M-T: 1.0  
M-E: 1.0
T-E: 1.0  
T-M: 1.0 
# A=B is cross reconstruction loss for modal A generating modality B
E=T: 1.0  
E=M: 1.0
M=T: 1.0  
M=E: 1.0
T=E: 1.0 
T=M: 1.0  

# Set SLURM parameters:
partition: 'celltypes'
cpus: 8
gpus: 1
nodes: 1
memory: '20g'
time: '100:00:00'
directory: '/allen/programs/celltypes/workgroups/mousecelltypes/MachineLearning/Ian/code/cplAE_MET'
conda: 'cplae'
