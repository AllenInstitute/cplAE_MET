{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from functools import reduce\n",
    "\n",
    "from cplAE_MET.utils.load_config import load_config\n",
    "from cplAE_MET.utils.analysis_tree_helpers import get_merged_types"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Set the config file name\n",
    "config_file = 'config_preproc.toml'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# function to set the path\n",
    "def set_paths(config_file=None):\n",
    " paths, _ = load_config(config_file=config_file, verbose=False)\n",
    " paths['input'] = f'{str(paths[\"data_dir\"])}'\n",
    " paths['arbor_density_file'] = f'{str(paths[\"arbor_density_file\"])}'\n",
    " paths['specimen_ids'] = f'{paths[\"input\"]}/{str(paths[\"specimen_ids_file\"])}'\n",
    " paths['anno'] = f'{paths[\"input\"]}/{str(paths[\"t_anno_output_file\"])}'\n",
    " paths['m_input'] = f'{paths[\"input\"]}/{str(paths[\"arbor_density_file\"])}'\n",
    " paths['t_input'] = f'{paths[\"input\"]}/{str(paths[\"t_data_output_file\"])}'\n",
    " paths['e_input'] = f'{paths[\"input\"]}/{str(paths[\"e_output_file\"])}'\n",
    " paths['met_output'] = f'{paths[\"input\"]}/{str(paths[\"met_output_file\"])}'\n",
    " paths['gene_id_input'] = f'{paths[\"input\"]}/{str(paths[\"gene_id_output_file\"])}'\n",
    " return paths"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Read T, E, M data\n",
    "dir_pth = set_paths(config_file=config_file)\n",
    "\n",
    "print(\"...................................................\")\n",
    "print(\"Loading specimens\")\n",
    "cells = pd.read_csv(dir_pth['specimen_ids'])\n",
    "print(\"Loading E, T and M data\")\n",
    "E_data = pd.read_csv(dir_pth['e_input'])\n",
    "print(\"shape of E data:\", E_data.shape)\n",
    "T_data = pd.read_csv(dir_pth['t_input'])\n",
    "print(\"shape of T data:\", T_data.shape)\n",
    "M_data = sio.loadmat(dir_pth['m_input'])\n",
    "print(\"shape of hist_ax_de_api_bas data:\", M_data['hist_ax_de_api_bas'].shape)\n",
    "# M_data = pd.read_csv(dir_pth['arbor_density_PC_file'])\n",
    "# print(\"shape of arbor density Scaled PC data:\", M_data.shape)\n",
    "# M_pc_vars = pd.read_csv(dir_pth['arbor_density_PC_vars_file'])\n",
    "gene_id = pd.read_csv(dir_pth['gene_id_input'])\n",
    "print(\"Loading T annotations\")\n",
    "T_ann = pd.read_csv(dir_pth['anno'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "...................................................\n",
      "Loading specimens\n",
      "Loading E, T and M data\n",
      "shape of E data: (16703, 83)\n",
      "shape of T data: (16703, 1253)\n",
      "shape of hist_ax_de_api_bas data: (16703, 120, 4, 4)\n",
      "Loading T annotations\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# To merge the data on the specimen id, we need to conver them to str \n",
    "print(\"...................................................\")\n",
    "print(\"read specimen ids from m data and align other with that\")\n",
    "M_data['specimen_id'] = [str(i) for i in M_data['specimen_id']]\n",
    "m_anno = pd.DataFrame({\"specimen_id\": np.array([mystr.rstrip() for mystr in M_data['specimen_id']])})\n",
    "M_data['specimen_id'] = [mystr.rstrip() for mystr in M_data['specimen_id']]\n",
    "m_anno['specimen_id'] = m_anno['specimen_id'].astype(str)\n",
    "E_data['specimen_id'] = E_data['specimen_id'].astype(str)\n",
    "T_data['specimen_id'] = T_data['specimen_id'].astype(str)\n",
    "cells['specimen_id'] = cells['specimen_id'].astype(str)\n",
    "T_ann['specimen_id'] = T_ann['specimen_id'].astype(str)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "...................................................\n",
      "read specimen ids from m data and align other with that\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# NOTE: investigate the following lines, you might want to remove the following line\n",
    "# We are dropping the features that have nans\n",
    "print(\"...................................................\")\n",
    "print(\"Drop bad features that have many nans befor combining\")\n",
    "t_feature_cols = [c for c in T_data.columns if c!=\"specimen_id\"]\n",
    "e_feature_cols = [c for c in E_data.columns if c!=\"specimen_id\"]\n",
    "feature_cols = t_feature_cols + e_feature_cols \n",
    "is_t_1d = np.all(~np.isnan(np.array(T_data[t_feature_cols])), axis=1)\n",
    "is_e_1d = np.all(~np.isnan(np.array(E_data[e_feature_cols])), axis=1)\n",
    "is_m_1d = np.all(~np.isnan(M_data['hist_ax_de_api_bas']), axis=(1,2,3))\n",
    "n_nan_per_col_t = np.sum(np.isnan(np.array(T_data[t_feature_cols])[is_t_1d]))\n",
    "n_nan_per_col_e = np.sum(np.isnan(np.array(E_data[e_feature_cols])[is_e_1d]))\n",
    "n_nan_per_col_m = np.sum(np.isnan(M_data['hist_ax_de_api_bas'][is_m_1d]))\n",
    "assert (n_nan_per_col_t == n_nan_per_col_e == n_nan_per_col_m == 0)\n",
    "#dropping the space at the end of the name of the cluster labels\n",
    "T_ann.loc[is_t_1d, 'Tree_first_cl_label'] = np.array([i.rstrip() for i in T_ann[is_t_1d]['Tree_first_cl_label'].to_list()])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "...................................................\n",
      "Drop bad features that have many nans befor combining\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(\"...................................................\")\n",
    "print(\"Combining M, E and T data and metadata\")\n",
    "# result = reduce(lambda left, right: pd.merge(left, right, on=['specimen_id'], how='left'), [m_anno, M_data])\n",
    "result = reduce(lambda left, right: pd.merge(left, right, on=['specimen_id'], how='left'), [m_anno, E_data])\n",
    "# result = reduce(lambda left, right: pd.merge(left, right, on=['specimen_id'], how='left'), [result, E_data])\n",
    "result = reduce(lambda left, right: pd.merge(left, right, on=['specimen_id'], how='left'), [result, T_data])\n",
    "result = reduce(lambda left, right: pd.merge(left, right, on=['specimen_id'], how='left'), [result, cells])\n",
    "result = reduce(lambda left, right: pd.merge(left, right, on=['specimen_id'], how='left'), [result, T_ann])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "...................................................\n",
      "Combining M, E and T data and metadata\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Remove all the cells that do not have all T, E and M ... none of the modalities at the same time \n",
    "idx = np.where(np.logical_and(~is_t_1d, np.logical_and(~is_e_1d, ~is_m_1d)))[0]\n",
    "idx = np.array([i for i in result.index if i not in idx])\n",
    "result = result.iloc[idx]\n",
    "M_data['hist_ax_de_api_bas'] = M_data['hist_ax_de_api_bas'][idx]\n",
    "\n",
    "is_t_1d = [v for i,v in enumerate(is_t_1d) if i in idx]\n",
    "is_e_1d = [v for i,v in enumerate(is_e_1d) if i in idx]\n",
    "is_m_1d = [v for i,v in enumerate(is_m_1d) if i in idx]\n",
    "print(\"number of t cells:\", np.array(is_t_1d).sum())\n",
    "print(\"number of e cells:\", np.array(is_e_1d).sum())\n",
    "print(\"number of m cells:\", np.array(is_m_1d).sum())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of t cells: 6552\n",
      "number of e cells: 7733\n",
      "number of m cells: 10246\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(\"...................................................\")\n",
    "print(\"Merging t types and writing the merged types in the mat file\")\n",
    "# First we need to find the cells that have T data available and then start merging them\n",
    "\n",
    "for nc in [40,50,60,70,80,90]:\n",
    "    name = \"merged_types_\" + str(nc)\n",
    "    merged_t, _, _ = get_merged_types(htree_file=\"/home/fahimehb/Local/new_codes/cplAE_MET/tree_20180520.csv\",  \n",
    "                                        cells_labels=np.array(result['Tree_first_cl_label'][is_t_1d]), \n",
    "                                        num_classes=nc,\n",
    "                                        ref_leaf=np.unique(result['Tree_first_cl_label'][is_t_1d]),\n",
    "                                        node=\"n1\")\n",
    "    result.loc[is_t_1d, name] = merged_t"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "...................................................\n",
      "Merging t types and writing the merged types in the mat file\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(\"...................................................\")\n",
    "print(\"Writing the output mat\")\n",
    "#writing M, E and T data\n",
    "model_input_mat = {}\n",
    "model_input_mat[\"E_dat\"] = np.array(result[[c for c in E_data.columns if c != \"specimen_id\"]])\n",
    "model_input_mat[\"T_dat\"] = np.array(result[[c for c in T_data.columns if c != \"specimen_id\"]])\n",
    "model_input_mat[\"M_dat\"] = M_data['hist_ax_de_api_bas']\n",
    "model_input_mat[\"soma_depth\"] = M_data['soma_depth']\n",
    "model_input_mat[\"hist_ax_de_api_bas\"] = M_data['hist_ax_de_api_bas']"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "...................................................\n",
      "Writing the output mat\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#writing the sample_ids and some meta data\n",
    "model_input_mat[\"specimen_id\"] = result['specimen_id'].to_list()\n",
    "model_input_mat['platform'] = result['platform'].to_list()\n",
    "model_input_mat[\"class\"] = result['class'].to_list()\n",
    "model_input_mat[\"group\"] = result['group'].to_list()\n",
    "model_input_mat[\"subgroup\"] = result['subgroup'].to_list()\n",
    "model_input_mat[\"class_id\"] = result['class_id'].to_list()\n",
    "model_input_mat[\"cluster_id\"] = result['Tree_first_cl_id'].to_list()\n",
    "model_input_mat[\"cluster_color\"] = result['Tree_first_cl_color'].to_list()\n",
    "model_input_mat[\"cluster_label\"] = result['Tree_first_cl_label'].to_list()\n",
    "model_input_mat[\"merged_cluster_label_at40\"] = result['merged_types_40'].to_list()\n",
    "model_input_mat[\"merged_cluster_label_at50\"] = result['merged_types_50'].to_list()\n",
    "model_input_mat[\"merged_cluster_label_at60\"] = result['merged_types_60'].to_list()\n",
    "model_input_mat[\"merged_cluster_label_at70\"] = result['merged_types_70'].to_list()\n",
    "model_input_mat[\"merged_cluster_label_at80\"] = result['merged_types_80'].to_list()\n",
    "model_input_mat[\"merged_cluster_label_at90\"] = result['merged_types_90'].to_list()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#Writing the E_feature and M_features names\n",
    "model_input_mat[\"gene_ids\"] = gene_id[\"gene_id\"].to_list()\n",
    "model_input_mat['E_features'] = [c for c in E_data.columns if c not in [\"specimen_id\"]]\n",
    "model_input_mat['M_features'] = [c for c in E_data.columns if c not in [\"specimen_id\"]] # I should remove this later"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#Saving input mat file for coupled autoencoder\n",
    "print(\"Size of M data:\", model_input_mat['M_dat'].shape)\n",
    "print(\"Size of E data:\", model_input_mat[\"E_dat\"].shape)\n",
    "print(\"Size of T data:\", model_input_mat[\"T_dat\"].shape)\n",
    "print(\"saving!\")\n",
    "\n",
    "sio.savemat(dir_pth['met_output'], model_input_mat)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of M data: (16703, 120, 4, 4)\n",
      "Size of E data: (16703, 82)\n",
      "Size of T data: (16703, 1252)\n",
      "saving!\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}